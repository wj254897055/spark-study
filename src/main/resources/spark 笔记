spark
----------------------
spark core
spark streaming
spark sql
spark mllib
spark graha

集群的部署模式
local
standlone
yarn
misos


RDD
resilient  distrbuted dataset 弹性分布式计算集
spark context
sc.textFile()  直接返回RDD


基于scala 语言开发


标签生成  D:\wujing\data\temptags.txt
00213 分量足（20） 环境好（1）

并行
        集群上同时多个节点同时计算
并发
        单个节点同时发起的请求的能力


三高
    高并发
    高负载
    高吞吐量
    ，


Rdd
----------------------
RDD 是弹性分布式数据库
    spark的基本抽象
    分区划的集合
    可用于并行计算

RDD内部5大属性
    分区列表
    计算每个切片的函数
    到其他RDD的依赖列表
    针对KV类型的RDD的分区类
    计算每个的首选的位置列表


RDD 常见操作  都是延迟计算的，只有调用action方法时，才会触发job

    1.  变换  返回新的RDD ->transform
        map
        filter
        flatmap
        mappartitons
        sample
        distinct
        intersection
        groupbykey ------->没有combine过程 能改变V的值
        reducebykey -------->有combine过程 不能改变v的值

        cogroup 协分组



    2.  action
            collect 产生action  执行运算
                foreachPartition

               repartition
                    底层调用的是coalesce shuffle=true
                    coalesce=再分区的核心方法，有机会指定是否shuffle,减少分区调用该方法
                    def coalesce(numPartitions: Int, shuffle: Boolean = false,//默认不分区
                                   partitionCoalescer: Option[PartitionCoalescer] = Option.empty)
                                  (implicit ord: Ordering[T] = null)


            按照Key聚合
            aggregateBykey   //mr一般过程


              def aggregateByKey[U: ClassTag](zeroValue: U)(seqOp: (U, V) => U,
                  combOp: (U, U) => U): RDD[(K, U)] = self.withScope {
                aggregateByKey(zeroValue, defaultPartitioner(self))(seqOp, combOp)
              }
              //map端聚合，分区內聚合

        count()
        take
        frist
        saveAsTextFile


        Spark api

            1.SparkConf
                spark配置对象，设置各种参数，KV形式
            2.SparkContext
                spark的主要入口，代表到spark集群的连接，可以创建RDD，累加器和广播变量

                JVM只有一个SparkContext  启动新的时，要stop之前的

                sc.textFile

            3.RDD
                有依赖列表
            4.Dependecy
                依赖：子RDD与父Rdd分区之间数量的对应关系
                    NarrowDependecy（窄依赖）
                        one-to-one 一对一
                        rang 范围
                        prune 修建依赖

                    ShuffleDependecy

             5.stage
                阶段是并行任务，由调度器运行的DAG图，根据Shuffle进行划分若干个Stage
                阶段分为两种
                   1.shuffleMapStage
                    该阶段的输出是下一阶段的输入跟踪每个节点输出
                    一个阶段会尝试执行多次处于容错考虑
                    由多个ShuffleMapTask构成
                   2.ResultStage
                    应用函数在某些分区上计算函数，有些操作没必要在所有节点上   frist
                    结果阶段的输出结果回传给driver
                    由多个ResultTask构成 

              6.task
                spark执行的最小单位
                    1.ResultTask
						执行任务，并将结果返回给driver					
                    2.ShuffleMapTask
						将RDD的元素分成多个桶


              7.ActiveJob
                    每个action 是一个job
					resultjob
					ShuffleJob
					
              8.application
                    一个应用对应多个Job 对应一个SparkContext                  
					
			  9.DAGScheduler
					DAGScheduler可以提交job 也可以提交stage  只发生在driver端
						高级调度器面向stage，负责为每个job计算stage的DAG,跟踪RDD和输出，以taskset的方式提交stage给下层的task调度器
						spark是以shuffle为边界将RDD划分为多个stage
						stage存储的前后关系
						
						DAGScheduler 会因为输出文件的丢失重复提交上一阶段的，其他原因导致的故障由task

					driver:调度框架采用三级调度机制
					
					DAGScheduler  面向stage
						|
					TaskScheduler  面向任务集 taskset
						|						
					BackendScheduler 
					
								
			
				10.TaskSchedulerImpl
				
				11.SchedulerBackend
				   impl-->CoarseGrainedSchedulerBackend
							
								-->StandaloneSchedulerBackend
							
						  LocalSchedulerBackend
				

Spark job的流程


	spark job运行分两步
	1.注册应用，分配资源
	2.提交job

https://www.jianshu.com/p/02a17ff44931

  Spark 应用程序被提交后，当某个动作算子触发了计算操作时，
  SparkContext 会向 DAGScheduler 提交一个作业，
  接着 DAGScheduler 会根据 RDD 生成的依赖关系划分 Stage，
  并决定各个 Stage 之间的依赖关系，Stage 之间的依赖关系就形成了 DAG。
  Stage 的划分是以 ShuffleDependency 为依据的，
  也就是说当某个 RDD 的运算需要将数据进行 Shuffle 时，
  这个包含了 Shuffle 依赖关系的 RDD 将被用来作为输入信息，
  进而构建一个新的 Stage。我们可以看到用这样的方式划分 Stage，
  能够保证有依赖关系的数据可以以正确的顺序执行。
  根据每个 Stage 所依赖的 RDD 数据的 partition 的分布，
  会产生出与 partition 数量相等的 Task，这些 Task 根据 partition 的位置进行分布。
  其次对于 finalStage 或是 mapStage 会产生不同的 Task，
  最后所有的 Task 会封装到 TaskSet 内提交到 TaskScheduler 去执行
  
  
  
  
  
 Spark 回顾
 ------------------------
 spark的核心组件
 
 
 sparkConfig  配置对象 KV形式
 sparkContext  入口 创建RDD 累加器和广播变量 
 stage     阶段 Rdd的链条
 task
 dependecy
 rdd
 job
 excutor
 
 
 
 job的部署模式
	1.client
		默认模式
		driver程序运行在client的节点
		
	2.cluster
		driver运行在某一个worker上  进程名称 DriverWrapper
		spark-shell 不能 以集群模式启动
		
	3.spark的执行模式
		spark-shell 不能 以集群模式启动
		spark-submit --master spark://xxx:7077 -deploy-mode client  (默认启动模式)
		
		spark-submit --master spark://xxx:7077 --class wordCount -deploy-mode cluster /xx.jar -参数


	4.spark yarn 模式
	yarn模式不存在spark集群，也不需要每个节点都去安装spark软件包
	只要在客户端安装spark即可，提交作业时，本质上是作为hadoop的一个job来执行
	执行流程和hadoop是一致的，只不过在NM启动的yarnChild的时候，启动的是spark的excutor
	
	yarn-client
		dirver 运行在client节点上,appMaster 只负责请求资源
	yarn-cluster
		driver 运行在appMaster上 APPmaster不但负责请求资源 还负责运行driver
		
		操作
		
		停止spark集群
		启动yarn
		考察webui
		提交job
		spark-submit --master yarn  --deploy-mode client  --class wordCount myspark.jar --参数
		
		
	5 为避免每次上传200m的spark包 性能过低
		将spark-job上传的到hdfs 
		配置spark，指定上传的文件
		


spark standlone 资源管理
	1.sparkwork默认资源分配
		cpu 
		memory 
		每个节点只启动一个worker实例
	2.配置spark的资源
		spark-env
		
		HADOOP_CONF_DIR
		
		#
		export SPARK_WORKER_CORES=4
		
		export SPARK_WORKER_MEMORY
		 
		export SPARK_DAEMON_MEMORY
		
	


Cluster deploy mode only:
  --driver-cores NUM          Number of cores used by the driver, only in cluster mode
                              (Default: 1).

 Spark standalone or Mesos with cluster deploy mode only:
  --supervise                 If given, restarts the driver on failure.
  --kill SUBMISSION_ID        If given, kills the driver specified.
  --status SUBMISSION_ID      If given, requests the status of the driver specified.

 Spark standalone and Mesos only:
  --total-executor-cores NUM  Total cores for all executors.

 Spark standalone and YARN only:
  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,
                              or all available cores on the worker in standalone mode)

 YARN-only:
  --queue QUEUE_NAME          The YARN queue to submit to (Default: "default").
  --num-executors NUM         Number of executors to launch (Default: 2).
                              If dynamic allocation is enabled, the initial number of
                              executors will be at least NUM.
  --archives ARCHIVES         Comma separated list of archives to be extracted into the
                              working directory of each executor.
  --principal PRINCIPAL       Principal to be used to login to KDC, while running on
                              secure HDFS.
  --keytab KEYTAB             The full path to the file that contains the keytab for the
                              principal specified above. This keytab will be copied to
                              the node running the Application Master via the Secure
                              Distributed Cache, for renewing the login tickets and the
                              delegation tokens periodically.



spark 资源管理
	1.master
	2.worker

sparkjob 的运行方式
	1.standlone+client
		启动spark集群 master worker
		driver运行在某一个clien上
		job提交方式分两步走  注册应用启动executor 提交job 在executor 启动task
		
	2.standlone+cluster
	3.yarn +client
		以hadoop作业的方式运行	
	4.yarn+cluster





广播变量
----------------------------------
	  import org.apache.spark.broadcast.Broadcast
	  
	  sparktask 对象携带有rdd和dep信息。这两部分是通过广播变量传播的
	  内部原理是将stage的rdd和dep构成二元组 通过序列化成字节数组
	  通过spark上下文，将字节数组包装成广播变量
	  
	  task对象关联的是广播变量
	------------------广播变量的机制-------------
	Broadcast  api
	广播变量将对象缓存到执行器上，避免多次拷贝
	
	
	
	TorrentBroadcast
	driver对串行化对象分割成小的chunk，并将这些chunk存放在driver的BlockManager
	在executor端，每个executor尝试在自己的blockmanager中抓取对象，如果不存在 在远程的driver/executor抓取小的chunk
	得到之后会存放到自己的blockmanager，也用于其他的executor的抓取
	可以防止driver想executor发送多次数据拷贝请求引发的瓶颈问题
	

    val bc = sc.broadcast("ca")
    bc.value
	
	
累加器	
------------------
	用于跟踪和调试技术
	在driver如果需要获取executor 的执行结果 ，可以使用累加机制
	executor端使用累加add方法，将结果回传给driver端，driver能实现结果
	
	
	
val acc=sc.longAccumulator("myacc")
 acc.add(e)//累加器
 
 
 自定义累加器
 -----------------------
 
 自定义气温累加
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 





















		
		





















